---
title: 'PM guiado por evals: el sistema operativo que falta'
date: '2026-02-25'
summary: >-
  Sin evaluaciones integradas en decisiones de producto, la IA se gestiona por
  intuición y se escala con fragilidad.
tags:
  - ai-pm
  - evaluation
  - product-ops
draft: true
---
En AI PM, la diferencia entre una demo atractiva y un producto durable suele estar en los detalles operativos que casi nunca se celebran en publico.

En muchos equipos, los evals viven en notebooks del equipo ML mientras producto decide roadmap, releases y promesas comerciales en documentos separados. Ese desacople crea un problema serio: decisiones de negocio se toman sin visibilidad real de calidad, seguridad y costo.

Si el producto depende del comportamiento del modelo, entonces evaluar ese comportamiento no es tarea periférica. Es parte del core operativo de PM.

Tecnicamente, la idea central es: los evals deben ser una entrada formal en el ciclo de decisión de producto, igual que métricas de adopción o retención.

Límite: no necesitas una infraestructura perfecta para empezar. Pero sí necesitas métricas de evaluación conectadas a decisiones concretas (despliegue, rollout, pricing, promesas de UX).

1. **Definir umbrales de valor**: qué calidad mínima necesita el usuario.
2. **Medir de forma recurrente**: calidad, seguridad, latencia, costo.
3. **Comparar contra thresholds**: no contra intuición.
4. **Tomar decisión operativa**: escalar, limitar, iterar o pausar.
5. **Retroalimentar al roadmap**: convertir gaps en prioridades.

Métricas útiles por categoría:

- Calidad: precisión en tareas representativas.
- Seguridad: tasa de outputs inaceptables.
- Latencia: percentiles en contexto real.
- Costo: costo por tarea completada con éxito.

Una startup de búsqueda semántica lanzaba mejoras cada semana basadas en feedback anecdótico. Usuarios reportaban “a veces mejor, a veces peor”. Sin baseline sólido, el equipo no sabía si avanzaba.

Crearon un set mínimo de evals por casos críticos de uso y bloquearon releases cuando caían por debajo de umbral. En pocas semanas, redujeron regresiones y pudieron explicar decisiones a stakeholders con evidencia.

El tradeoff tecnico-operativo es: menor número de releases al inicio. Beneficio: releases con mayor confianza y menos rollback.

“Los evals son lentos y nos frenan.”

Desde producto y riesgo, la respuesta es: evals mal diseñados sí frenan. Evals enfocados aceleran porque evitan ciclos de prueba-error ciegos en producción. El verdadero freno es lanzar sin señales confiables y corregir bajo presión.

- Define 3 métricas de eval ligadas a valor de usuario.
- Establece umbrales mínimos para avanzar un despliegue.
- Incluye estado de evals en la review semanal de producto.
- Vincula cada degradación de eval a un ítem de roadmap.
- Alinea a GTM en qué claims dependen de qué métricas.

La clave no es tener más métricas; es conectar métricas con decisiones concretas. Define una tabla simple para cada funcionalidad de IA: métrica, umbral mínimo, frecuencia de medición, decisión asociada si cae por debajo del umbral. Esto evita dashboards decorativos.

Ejemplo práctico:

- Calidad de respuesta >= 85% en casos críticos.
- Latencia p95 <= 4s en horario pico.
- Costo por tarea <= umbral de margen definido.

Si una métrica cae, la decisión está preacordada: limitar rollout, activar plan de respaldo, o pausar promoción comercial.

Otro patrón útil es separar evals offline y señales online. Offline te da control experimental; online te da verdad de uso real. Ambos son necesarios. Equipos que solo miran offline se sorprenden en producción. Equipos que solo miran online aprenden tarde.

Además, agrega una revisión mensual de "deuda de evaluación": casos de uso no cubiertos, segmentos sin datos, métricas no representativas. Igual que deuda técnica, esta deuda crece silenciosamente y luego frena toda iteración.

En términos culturales, PM debe preguntar en cada decisión relevante: "¿qué evidencia de eval respalda este cambio?". No para burocratizar, sino para elevar calidad de conversación entre producto, ingeniería y negocio.

Evals no reemplazan criterio de producto. Lo hacen más trazable, menos político y más repetible.

Instala una review fija de 45 minutos con formato estable: métricas que mejoraron, métricas que empeoraron, decisiones tomadas, decisiones pendientes. La estabilidad del formato reduce ruido y acelera alineación. Evita convertirla en reunión de status; debe terminar con decisiones concretas de producto y dueños asignados.

Para equipos pequeños, basta con una hoja compartida y disciplina semanal. La sofisticación de tooling puede llegar después.
